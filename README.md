# üß† 1. Introdu√ß√£o ao PyTorch

## ‚ú≥Ô∏è O que √© PyTorch?

PyTorch √© um framework **open-source** para deep learning e computa√ß√£o num√©rica com foco 
]em **flexibilidade**, **performance** e **usabilidade**. Baseado em **Torch** (Lua), foi 
desenvolvido pelo **Facebook AI Research Lab (FAIR)** e lan√ßado em **2016**.

---

# üï∞Ô∏è 2. Breve Hist√≥ria e Motiva√ß√µes

| Ano  | Evento chave                                                     |
|------|------------------------------------------------------------------|
| 2011 | Torch (em Lua) utilizado por DeepMind e Facebook                 |
| 2015 | TensorFlow (Google) √© lan√ßado (substituindo o Theano)            |
| 2016 | Lan√ßamento do PyTorch (baseado em Torch + Python)                |
| 2019 | PyTorch se torna padr√£o no FAIR e √© adotado por MS, AWS          |
| 2022 | PyTorch √© doado √† Linux Foundation e se torna PyTorch Foundation |

### üéØ Motiva√ß√µes do PyTorch:

- Simplicidade e leitura de c√≥digo semelhante a **NumPy**
- **Dynamic computation graph (define-by-run)** ‚Äî f√°cil de depurar
- Forte integra√ß√£o com o **ecossistema Python**
- Facilita **pesquisa e prototipagem**

---

# üìä 3. Comparativo PyTorch vs TensorFlow 2

| Crit√©rio              | PyTorch                        | TensorFlow 2                        |
|-----------------------|--------------------------------|-------------------------------------|
| Paradigma de execu√ß√£o | Din√¢mico (define-by-run)       | Est√°tico (Graph) + modo eager       |
| Sintaxe               | Python puro / NumPy-like       | API pr√≥pria, mais verbosa           |
| Curva de aprendizado  | R√°pida                         | Mais √≠ngreme                        |
| Debugging             | Simples com `print()` ou `pdb` | Precisa de `tf.print` ou `tf.debug` |
| Comunidade acad√™mica  | Muito forte                    | Mais adotado na ind√∫stria           |
| Ferramentas de deploy | TorchScript, ONNX              | TF Serving, TFLite                  |

---

# üåê 4. PyTorch no mercado

- **Empresas que usam**: Facebook, Tesla, Microsoft, OpenAI, Amazon, NVIDIA, Hugging Face, Stability AI
- **Domina em pesquisa**: arXiv, NeurIPS, ICLR, ICML
- **√Åreas de destaque**:
  - Vis√£o computacional
  - Processamento de linguagem natural (NLP)
  - Modelos generativos (GANs, Diffusion)
  - **LLMs** (Large Language Models)

# üì¶ Instala√ß√£o
Para instalar o Pytorch usando o PIP, basta executar o comando

```
$ pip install torch torchvision
```

---

# üì¶ O se deve saber sobre **Tensores**

## üîç O que s√£o tensores?

Tensores s√£o **estruturas de dados** fundamentais em Deep Learning ‚Äî s√£o a base de todo o processamento em bibliotecas como **PyTorch**, **TensorFlow** e outras.

> Em ess√™ncia, um **tensor √© uma generaliza√ß√£o de matrizes e vetores** para m√∫ltiplas dimens√µes.

| Objeto Matem√°tico | Representa√ß√£o Tensorial | Exemplo com PyTorch              |
|-------------------|-------------------------|----------------------------------|
| Escalar (n√∫mero)  | Tensor 0D               | `torch.tensor(3.14)`             |
| Vetor             | Tensor 1D               | `torch.tensor([1.0, 2.0, 3.0])`  |
| Matriz            | Tensor 2D               | `torch.tensor([[1, 2], [3, 4]])` |
| Cubo de dados     | Tensor 3D               | `torch.randn(2, 3, 4)`           |
| Volume N-D        | Tensor N-dimensional    | `torch.randn(10, 3, 32, 32)`     |

---

## üß† Por que tensores s√£o importantes?

- Eles **representam os dados de entrada e sa√≠da** de redes neurais (imagens, textos, s√©ries temporais, etc).
- Todos os **par√¢metros dos modelos (pesos e bias)** tamb√©m s√£o tensores.
- O **processo de backpropagation** (retropropaga√ß√£o) utiliza tensores e opera√ß√µes entre eles.
- S√£o otimizados para **opera√ß√µes paralelas em GPU**.

---

## üìè Importante: Tensores ‚â† Arrays
Apesar de parecerem arrays do NumPy, tensores possuem vantagens importantes:

Autograd: registram opera√ß√µes para c√°lculo autom√°tico de derivadas

Compatibilidade com GPU

Opera√ß√µes otimizadas para Deep Learning


## ‚öôÔ∏è Criando tensores com PyTorch

```python
import torch

# Tensor a partir de lista
a = torch.tensor([1.0, 2.0, 3.0])

# Tensor com valores aleat√≥rios
b = torch.randn(2, 3)  # 2 linhas, 3 colunas

# Tensor com todos os zeros
z = torch.zeros(4, 4)

# Tensor com todos os uns
o = torch.ones(2, 2)
```
## üßÆ Opera√ß√µes com tensores

Tensores suportam **opera√ß√µes matem√°ticas** similares ao NumPy:

```python
import torch

x = torch.tensor([1.0, 2.0])
y = torch.tensor([3.0, 4.0])

print(x + y)        # Soma
print(x * y)        # Multiplica√ß√£o, elemento a elemento
print(x @ y)        # Produto escalar
```


## üöÄ Tensores na GPU

Fundamental para aproveitar o paralelismo das GPUs, acelerando o treinamento de redes neurais.

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x = torch.tensor([1.0, 2.0], device=device)
```

## üìê Forma (shape) e dimens√£o (rank)

```python
t = torch.randn(3, 4, 5)  # Tensor 3D

print(t.shape)    # (3, 4, 5)
print(t.ndim)     # 3 (dimens√µes)
```

| Termo	  | Significado                      |
|---------|----------------------------------|
| shape	  | Quantidade de elementos por eixo |
| ndim	   | N√∫mero de eixos (dimens√µes)      |
| size()	 | Alternativa a shape              |

# üîÅ Broadcasting
PyTorch permite opera√ß√µes entre tensores com formas diferentes, desde que sejam compat√≠veis:

```python
A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([10, 20])

print(A + B)  # B √© "expandido" automaticamente
```


---
